# KV Caching From Scratch Pytorch

Last week, I explored KV caching from scratch while working on a GPT-2 model. I ran my experiments on a Colab T4 GPU to better understand how caching improves inference speed in large language models.

## ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º?
In autoregressive generation, LLMs generate one token at a time, and each new token has to attend to all previous tokens.
So if your model generated: "White â†’ Fluffy â†’ Cat"
â€¦the attention block still recomputes the Keys and Values for "White" and "Fluffy" every time.

Thatâ€™s a lot of unnecessary computation, especially as the output grows longer.

## ğ—§ğ—µğ—² ğ—ğ—© ğ—–ğ—®ğ—°ğ—µğ—² ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»:
I implemented a caching mechanism where:
 â€¢ The model caches the Keys & Values for input tokens during prefill.
 â€¢ For each new token, it only computes the K/V for that token.
 â€¢ Previous tokens just pull from the cache, no recompute needed.

 ![attention_cache](https://github.com/user-attachments/assets/08e95b8c-9616-4095-8c48-f4d750fe385f)


## ğŸ“Š ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€:
Tested on a Colab T4 GPU for GPT-2 using different output lengths:

![KV Cache](https://github.com/user-attachments/assets/33c5ce39-fd43-4c74-9f01-daac0afa13a0)


## ğ—¢ğ—»ğ—² ğ—™ğ˜‚ğ—» ğ—œğ—»ğ˜€ğ—¶ğ—´ğ—µğ˜:
For shorter outputs, KV caching doesn't always help.
In my tests, device communication overhead on CUDA sometimes outweighed the gains for small models like GPT-2.

Shoutout to Sebastian Raschka, PhD for his amazing [blog](https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms) post on KV Caching

## References

1. [Attention in Transformers, Step-by-Step | Deep Learning Chapter 6](https://www.youtube.com/watch?v=eMlx5fFNoYc)
2. [Understanding and Coding the KV Cache in LLMs from Scratch](https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms)
3. [Mastering Tensor Dimensions in Transformers](https://huggingface.co/blog/not-lain/tensor-dims)
4. [The Illustrated Transformer â€“ Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
5. [Transformers KV Caching Explained | JoÃ£o Lages](https://medium.com/@joaolages/kv-caching-explained-276520203249)
6. [LLM Inference Series: 3. KV Caching Explained | Pierre Lienhart](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8)
7. [tanishqkumar/beyond-nanogpt: Minimal, annotated implementations](https://github.com/tanishqkumar/beyond-nanogpt)

